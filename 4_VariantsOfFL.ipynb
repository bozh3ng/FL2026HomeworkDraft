{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d2e349",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afa4b2",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "(Single Choice) In horizontal federated learning (HFL), participating organizations primarily differ in \n",
    "\n",
    "a) the set of features they hold \n",
    "\n",
    "b) the set of samples they hold\n",
    "\n",
    "c) the training algorithm \n",
    "\n",
    "d) the encryption scheme\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edcf419",
   "metadata": {},
   "source": [
    "## 2\n",
    "Consider these two scenarios:\n",
    "\n",
    "1. Several hospitals in different cities each keep the same set of clinical variables (age, sex, lab tests) but for different patients.\n",
    "2. A commercial bank and an e-commerce platform both serve many of the same customers: the bank owns account balances and credit scores, while the retailer owns browsing and purchase histories.\n",
    "\n",
    "which scenario is suitable of HFL and which is suitable for VFL , and why \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c23a0",
   "metadata": {},
   "source": [
    "## 3\n",
    "Explain briefly why data heterogeneity across clients can slow convergence in horizontal FL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f85b1",
   "metadata": {},
   "source": [
    "# Coding\n",
    "\n",
    "We try to implement a single-model FL example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea3c6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 01: MSE = 5.3397\n",
      "Round 02: MSE = 3.5184\n",
      "Round 03: MSE = 2.3250\n",
      "Round 04: MSE = 1.5410\n",
      "Round 05: MSE = 1.0248\n",
      "Round 06: MSE = 0.6841\n",
      "Round 07: MSE = 0.4589\n",
      "Round 08: MSE = 0.3096\n",
      "Round 09: MSE = 0.2105\n",
      "Round 10: MSE = 0.1445\n",
      "Round 11: MSE = 0.1005\n",
      "Round 12: MSE = 0.0712\n",
      "Round 13: MSE = 0.0515\n",
      "Round 14: MSE = 0.0383\n",
      "Round 15: MSE = 0.0295\n",
      "Round 16: MSE = 0.0235\n",
      "Round 17: MSE = 0.0195\n",
      "Round 18: MSE = 0.0168\n",
      "Round 19: MSE = 0.0149\n",
      "Round 20: MSE = 0.0137\n",
      "\n",
      "True weights :  [1.5409960746765137, -0.293428897857666, -2.1787893772125244, 0.5684312582015991, -1.0845223665237427]\n",
      "Learned weights: [1.5136923789978027, -0.2985497713088989, -2.133829355239868, 0.5473030805587769, -1.0536893606185913]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Server-based (centralized) Federated Learning demo\n",
    "--------------------------------------------------\n",
    "• Synthetic linear-regression data for N clients\n",
    "• One global model (simple Linear without bias)\n",
    "• FedAvg aggregation on the server\n",
    "\"\"\"\n",
    "\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from copy import deepcopy\n",
    "\n",
    "# ------------------------------ hyper-parameters ------------------------------\n",
    "NUM_CLIENTS      = 3        # number of edge devices\n",
    "SAMPLES_PER_CL   = 100      # local data size\n",
    "N_FEATURES       = 5        # dimensionality of x\n",
    "ROUNDS           = 20       # FedAvg communication rounds\n",
    "LOCAL_EPOCHS     = 1        # SGD passes per client per round\n",
    "LR               = 0.1      # local learning rate\n",
    "SEED             = 0        # reproducibility\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ------------------------------ synthetic data -------------------------------\n",
    "true_w = torch.randn(N_FEATURES, 1)           # unknown ground-truth weights\n",
    "\n",
    "clients_data = []\n",
    "for _ in range(NUM_CLIENTS):\n",
    "    X = torch.randn(SAMPLES_PER_CL, N_FEATURES)\n",
    "    y = X @ true_w + 0.1 * torch.randn(SAMPLES_PER_CL, 1)  # noisy labels\n",
    "    clients_data.append((X, y))\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------ model definition -----------------------------\n",
    "class LinearModel(nn.Module):\n",
    "    \"\"\"Single-layer linear regression w/o bias to keep things simple.\"\"\"\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.zeros(d, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.w\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------ FedAvg helper --------------------------------\n",
    "def fed_avg(state_dicts, sizes):\n",
    "    \"\"\"Weighted (by sample count) parameter averaging.\"\"\"\n",
    "    new_state = deepcopy(state_dicts[0])\n",
    "    for k in new_state:\n",
    "        new_state[k].data.zero_()\n",
    "    total = float(sum(sizes))\n",
    "    for st, n in zip(state_dicts, sizes):\n",
    "        for k in st:\n",
    "            new_state[k] += st[k] * (n / total)\n",
    "    return new_state\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------ training loop --------------------------------\n",
    "global_model = LinearModel(N_FEATURES)        # w^(0)\n",
    "\n",
    "for rnd in range(ROUNDS):\n",
    "    client_states, sizes = [], []\n",
    "\n",
    "    # ----------- step 1: server → clients (broadcast w^(k)) -----------\n",
    "    for X, y in clients_data:\n",
    "        # copy global model to the client\n",
    "        client = LinearModel(N_FEATURES)\n",
    "        client.load_state_dict(global_model.state_dict())\n",
    "\n",
    "        # ----------- step 2: local update (compute w^(i,k)) -----------\n",
    "        opt = optim.SGD(client.parameters(), lr=LR)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        for _ in range(LOCAL_EPOCHS):\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(client(X), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # send update back to server\n",
    "        client_states.append(client.state_dict())\n",
    "        sizes.append(len(X))\n",
    "\n",
    "    # ----------- step 3: server aggregation (FedAvg) ------------------\n",
    "    new_state = fed_avg(client_states, sizes)\n",
    "    global_model.load_state_dict(new_state)   # w^(k+1)\n",
    "\n",
    "    # optional: monitor global loss\n",
    "    with torch.no_grad():\n",
    "        mse, total = 0.0, 0\n",
    "        for X, y in clients_data:\n",
    "            mse += ((global_model(X) - y) ** 2).sum().item()\n",
    "            total += len(X)\n",
    "        print(f\"Round {rnd+1:02d}: MSE = {mse / total:.4f}\")\n",
    "\n",
    "# ------------------------------ results --------------------------------------\n",
    "print(\"\\nTrue weights : \", true_w.squeeze().tolist())\n",
    "print(\"Learned weights:\", global_model.w.squeeze().tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
